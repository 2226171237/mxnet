{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**接着mxnet_deeplearning file**<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "## 批量归一化\n",
    "<font size=4>**主要让收敛的更快，对精度提升不大**</font><br/>\n",
    "    \n",
    "本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易 [1]。在 “实战Kaggle比赛：预测房价” 一节里，我们对输入数据做了标准化处理：处理后的**任意一个特征**在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。\n",
    "\n",
    "通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。\n",
    "\n",
    "批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量归一化层\n",
    "对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。\n",
    "#### 对全连接层做批量归一化\n",
    "通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为u，权重参数和偏差参数分别为W和b，激活函数为ϕ。设批量归一化的运算符为BN。那么，使用批量归一化的全连接层的输出为\n",
    "\n",
    "$ϕ(BN(x))$,<br/>\n",
    "其中批量归一化输入x由仿射变换\n",
    "\n",
    "$x=Wu+b$<br/>\n",
    "得到。考虑一个由m个样本组成的小批量，仿射变换的输出为一个新的小批量$B={x^{(1)},…,x^{(m)}}$。它们正是批量归一化层的输入。对于小批量B中任意样本$x^{(i)}∈R^d,1≤i≤m$，批量归一化层的输出同样是d维向量$y^{(i)}=BN(x^{(i)})$,\n",
    "\n",
    "并由以下几步求得。首先，对小批量B求均值和方差：\n",
    "\n",
    "$μ_B←\\frac{1}{m} \\sum_{i=1}^m x^{(i)}$,<br/>\n",
    "$σ_B^2←\\frac{1}{m} \\sum_{i=1}^m (x^{(i)}-\\mu_B)^2$,<br/>\n",
    "其中的平方计算是按元素求平方。接下来，使用按元素开方和按元素除法对$x^{(i)}$标准化：\n",
    "\n",
    "$\\hat{x}^{(i)}←\\frac{x^{(i)}−μ_B}{\\sqrt{σ_B^2+ϵ}}$,<br/>\n",
    "这里ϵ>0是一个很小的常数，保证分母大于0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 γ 和偏移（shift）参数 β。这两个参数和$x^{(i)}$形状相同，皆为d维向量。它们与$x^{(i)}$分别做按元素乘法（符号⊙）和加法计算：\n",
    "\n",
    "$y^{(i)}←γ⊙\\hat{x}^{(i)}+β$.<br/>\n",
    "至此，我们得到了$x^{(i)}$的批量归一化的输出$y^{(i)}$。 值得注意的是，可学习的拉伸和偏移参数保留了不对$\\hat{x}^{(i)}$做批量归一化的可能：此时只需学出$γ=\\sqrt{σ_B^2+ϵ}$和$β=\\mu_B$。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对卷积层做批量归一化\n",
    "对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。设小批量中有 m 个样本。在单个通道上，假设卷积计算输出的高和宽分别为 p 和 q 。我们需要对该通道中 m×p×q 个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 m×p×q 个元素的均值和方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测时的批量归一化\n",
    "使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import autograd,nd,init,gluon\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    if not autograd.is_training():\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat=(X-moving_mean)/nd.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        if len(X.shape)==2:\n",
    "            #使用全连接情况\n",
    "            mean=X.mean(axis=0)#求关于那个维度的均值就不写哪个维度。其余的都写,求各个特征的均值\n",
    "            var=((X-mean)**2).mean(axis=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean=X.mean(axis=(0,2,3),keepdims=True)#求关于那个维度的均值就不写哪个维度。其余的都写\n",
    "            var=((X-mean)**2).mean(axis=(0,2,3),keepdims=True)\n",
    "        X_hat=(X-mean)/nd.sqrt(var+eps)\n",
    "        #更新移动的平均值和方差\n",
    "        moving_mean=momentum*moving_mean+(1-momentum)*mean\n",
    "        moving_var=momentum*moving_var+(1-momentum)*var\n",
    "    Y=gamma*X_hat+beta\n",
    "    return Y,moving_mean,moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。BatchNorm实例所需指定的**num_features参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数**。该实例所需指定的num_dims参数对于全连接层和卷积层来说分别为2和4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self,num_features,num_dims,**kwargs):\n",
    "        super(BatchNorm,self).__init__(**kwargs)\n",
    "        if num_dims==2:\n",
    "            shape=(1,num_features)\n",
    "        else:\n",
    "            shape=(1,num_features,1,1)\n",
    "        self.gamma=self.params.get('gamma',shape=shape,init=init.One())\n",
    "        self.beta=self.params.get('beta',shape=shape,init=init.Zero())\n",
    "        self.moving_mean=nd.zeros(shape)\n",
    "        self.moving_var=nd.zeros(shape)\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.context!=X.context:\n",
    "            self.moving_mean=self.moving_mean.copyto(X.context)\n",
    "            self.moving_var=self.moving_var.copyto(X.context)\n",
    "         # 保存更新过的moving_mean和moving_var\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(X,self.gamma.data(),self.beta.data(),self.moving_mean,self.moving_var,eps=1e-5,momentum=0)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用批量归一化层的LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(6,kernel_size=5))\n",
    "net.add(BatchNorm(6,num_dims=4))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "net.add(nn.Conv2D(16,kernel_size=5))\n",
    "net.add(BatchNorm(16,num_dims=4))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "net.add(nn.Dense(120))\n",
    "net.add(BatchNorm(120,num_dims=2))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.Dense(84))\n",
    "net.add(BatchNorm(84,num_dims=2))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.6452, train acc 0.770, test acc 0.857, time 9.0 sec\n",
      "epoch 2, loss 0.3974, train acc 0.855, test acc 0.865, time 8.8 sec\n",
      "epoch 3, loss 0.3438, train acc 0.875, test acc 0.873, time 8.7 sec\n",
      "epoch 4, loss 0.3183, train acc 0.884, test acc 0.881, time 8.9 sec\n",
      "epoch 5, loss 0.3008, train acc 0.891, test acc 0.887, time 8.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs,batch_size,ctx=1.0,5,256,d2l.try_gpu()\n",
    "net.initialize(ctx=ctx,init=init.Xavier(),force_reinit=True)\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1.825248  1.9441062 0.9561216 1.7321875 2.191279  1.5300164]\n",
       " <NDArray 6 @gpu(0)>, \n",
       " [-0.16317885  0.24874806 -0.77239984 -0.20372763 -2.2464998  -0.24971762]\n",
       " <NDArray 6 @gpu(0)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.data().reshape((-1,)),net[1].beta.data().reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(6,kernel_size=5))\n",
    "net.add(nn.BatchNorm())\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "net.add(nn.Conv2D(16,kernel_size=5))\n",
    "net.add(nn.BatchNorm())\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "net.add(nn.Dense(120))\n",
    "net.add(nn.BatchNorm())\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.Dense(84))\n",
    "net.add(nn.BatchNorm())\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.6641, train acc 0.765, test acc 0.805, time 5.6 sec\n",
      "epoch 2, loss 0.4036, train acc 0.854, test acc 0.846, time 5.4 sec\n",
      "epoch 3, loss 0.3521, train acc 0.873, test acc 0.866, time 5.7 sec\n",
      "epoch 4, loss 0.3241, train acc 0.883, test acc 0.872, time 5.5 sec\n",
      "epoch 5, loss 0.3067, train acc 0.888, test acc 0.889, time 5.4 sec\n"
     ]
    }
   ],
   "source": [
    "net.initialize(ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.7263, train acc 0.757, test acc 0.836, time 5.5 sec\n",
      "epoch 2, loss 0.4002, train acc 0.856, test acc 0.870, time 5.4 sec\n",
      "epoch 3, loss 0.3496, train acc 0.873, test acc 0.827, time 5.4 sec\n",
      "epoch 4, loss 0.3219, train acc 0.883, test acc 0.873, time 5.5 sec\n",
      "epoch 5, loss 0.3009, train acc 0.891, test acc 0.874, time 5.4 sec\n"
     ]
    }
   ],
   "source": [
    "lr=1.5\n",
    "net.initialize(ctx=ctx,init=init.Xavier(),force_reinit=True)\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 尝试一下不学习拉伸参数gamma和偏移参数beta（构造的时候加入参数grad_req='null'来避免计算梯度），观察并分析结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self,num_features,num_dims,**kwargs):\n",
    "        super(BatchNorm,self).__init__(**kwargs)\n",
    "        if num_dims==2:\n",
    "            shape=(1,num_features)\n",
    "        else:\n",
    "            shape=(1,num_features,1,1)\n",
    "        self.gamma=self.params.get('gamma',shape=shape,init=init.One())\n",
    "        self.gamma.grad_req='null'  #对gamma不求梯度\n",
    "        self.beta=self.params.get('beta',shape=shape,init=init.Zero())\n",
    "        self.beta.grad_req='null'\n",
    "        self.moving_mean=nd.zeros(shape)\n",
    "        self.moving_var=nd.zeros(shape)\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.context!=X.context:\n",
    "            self.moving_mean=self.moving_mean.copyto(X.context)\n",
    "            self.moving_var=self.moving_var.copyto(X.context)\n",
    "         # 保存更新过的moving_mean和moving_var\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(X,self.gamma.data(),self.beta.data(),self.moving_mean,self.moving_var,eps=1e-5,momentum=0)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.6544, train acc 0.769, test acc 0.849, time 8.0 sec\n",
      "epoch 2, loss 0.4202, train acc 0.849, test acc 0.871, time 8.0 sec\n",
      "epoch 3, loss 0.3697, train acc 0.868, test acc 0.877, time 8.0 sec\n",
      "epoch 4, loss 0.3416, train acc 0.880, test acc 0.884, time 8.0 sec\n",
      "epoch 5, loss 0.3205, train acc 0.885, test acc 0.888, time 8.1 sec\n"
     ]
    }
   ],
   "source": [
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(6,kernel_size=5))\n",
    "net.add(BatchNorm(6,num_dims=4))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "net.add(nn.Conv2D(16,kernel_size=5))\n",
    "net.add(BatchNorm(16,num_dims=4))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "net.add(nn.Dense(120))\n",
    "net.add(BatchNorm(120,num_dims=2))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.Dense(84))\n",
    "net.add(BatchNorm(84,num_dims=2))\n",
    "net.add(nn.Activation('sigmoid'))\n",
    "net.add(nn.Dense(10))\n",
    "\n",
    "lr,num_epochs,batch_size,ctx=1.0,5,256,d2l.try_gpu()\n",
    "net.initialize(ctx=ctx,init=init.Xavier(),force_reinit=True)\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 1. 1. 1. 1. 1.]\n",
       " <NDArray 6 @gpu(0)>, \n",
       " [0. 0. 0. 0. 0. 0.]\n",
       " <NDArray 6 @gpu(0)>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.data().reshape((-1,)),net[1].beta.data().reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用重复元素的网络(VGG)\n",
    "AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。我们将在本章的后续几节里介绍几种不同的深度网络设计思路。\n",
    "\n",
    "本节介绍VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group [1]。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。\n",
    "### VGG块\n",
    "VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接上一个步幅为2、窗口形状为 2×2 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量num_convs和输出通道数num_channels。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import init,gluon,nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def vgg_block(num_convs,num_channels):\n",
    "    blk=nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        blk.add(nn.Conv2D(num_channels,kernel_size=3,padding=1,activation='relu'))\n",
    "    blk.add(nn.MaxPool2D(pool_size=2,strides=2))\n",
    "    return blk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG网络\n",
    "与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则跟AlexNet中的一样。\n",
    "\n",
    "现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。\n",
    "\n",
    "![VGGNet不同配置](https://img-blog.csdn.net/20170715114221637?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDI4MTM5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch=((1,64),(1,128),(2,256),(2,512),(2,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net=nn.Sequential()\n",
    "    #卷积部分\n",
    "    for (num_convs,num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs,num_channels))\n",
    "    #全连接层\n",
    "    net.add(nn.Dense(4096,activation='relu'))\n",
    "    net.add(nn.Dropout(0.5))\n",
    "    net.add(nn.Dense(4096,activation='relu'))\n",
    "    net.add(nn.Dropout(0.5))\n",
    "    net.add(nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "net=vgg(conv_arch=conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential1 out shape: (1, 64, 112, 112)\n",
      "sequential2 out shape: (1, 128, 56, 56)\n",
      "sequential3 out shape: (1, 256, 28, 28)\n",
      "sequential4 out shape: (1, 512, 14, 14)\n",
      "sequential5 out shape: (1, 512, 7, 7)\n",
      "dense0 out shape: (1, 4096)\n",
      "dropout0 out shape: (1, 4096)\n",
      "dense1 out shape: (1, 4096)\n",
      "dropout1 out shape: (1, 4096)\n",
      "dense2 out shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "net.initialize(force_reinit=True)\n",
    "X=nd.random.uniform(shape=(1,1,224,224))\n",
    "for blk in net:\n",
    "    X=blk(X)\n",
    "    print(blk.name,'out shape:',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成4后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据和训练模型\n",
    "因为VGG-11计算上比AlexNet更加复杂，出于测试的目的我们构造一个通道数更小，或者说更窄的网络在Fashion-MNIST数据集上进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 8), (1, 16), (2, 32), (2, 64), (2, 64)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio=8\n",
    "small_conv_arch=[(pair[0],pair[1]//ratio) for pair in conv_arch]\n",
    "small_conv_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.8286, train acc 0.701, test acc 0.841, time 456.0 sec\n",
      "epoch 2, loss 0.4055, train acc 0.853, test acc 0.882, time 450.3 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a6ae8094835a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data_fashion_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ch5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Machine_learning\\mxnet\\d2lzh.py\u001b[0m in \u001b[0;36mtrain_ch5\u001b[1;34m(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\liyajie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2011\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2013\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2015\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\liyajie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1993\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr,num_epochs,batch_size,ctx=0.05,5,128,d2l.try_gpu()\n",
    "net=vgg(small_conv_arch)\n",
    "net.initialize(ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size,resize=224)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络中的网络（NiN）\n",
    "前几节介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n",
    "\n",
    "![](https://images2017.cnblogs.com/blog/606386/201710/606386-20171011104028715-922103502.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NiN块\n",
    "我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。回忆在“多输入通道和多输出通道”一节里介绍的 1×1 卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用 1×1 卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。左图是AlexNet和VGG的网络结构局部，右图是NiN的网络结构局部。\n",
    "\n",
    "![](http://zh.d2l.ai/_images/nin.svg)\n",
    "\n",
    "NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的 1×1 卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import gluon,init,nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(num_channels,kernel_size,strides,padding):\n",
    "    blk=nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels,kernel_size=kernel_size,strides=strides,padding=padding,activation='relu'))\n",
    "    blk.add(nn.Conv2D(num_channels,kernel_size=1,activation='relu'))\n",
    "    blk.add(nn.Conv2D(num_channels,kernel_size=1,activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NiN模型\n",
    "NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为 11×11 、 5×5 和 3×3 的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为 3×3 的最大池化层。\n",
    "\n",
    "除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential()\n",
    "net.add(nin_block(96,kernel_size=11,strides=4,padding=0))\n",
    "net.add(nn.MaxPool2D(pool_size=3,strides=2))\n",
    "net.add(nin_block(256,kernel_size=5,strides=1,padding=2))\n",
    "net.add(nn.MaxPool2D(pool_size=3,strides=2))\n",
    "net.add(nin_block(384,kernel_size=3,strides=1,padding=1))\n",
    "net.add(nn.MaxPool2D(pool_size=3,strides=2))\n",
    "net.add(nn.Dropout(0.5))\n",
    "# 类别数为10\n",
    "net.add(nin_block(10,kernel_size=3,strides=1,padding=1))\n",
    "# 全局平均池化层将窗口形状自动设置成输入的高和宽\n",
    "net.add(nn.GlobalAvgPool2D())\n",
    "net.add(nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential1 output shape: (1, 96, 54, 54)\n",
      "pool0 output shape: (1, 96, 26, 26)\n",
      "sequential2 output shape: (1, 256, 26, 26)\n",
      "pool1 output shape: (1, 256, 12, 12)\n",
      "sequential3 output shape: (1, 384, 12, 12)\n",
      "pool2 output shape: (1, 384, 5, 5)\n",
      "dropout0 output shape: (1, 384, 5, 5)\n",
      "sequential4 output shape: (1, 10, 5, 5)\n",
      "pool3 output shape: (1, 10, 1, 1)\n",
      "flatten0 output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X=nd.random.uniform(shape=(1,1,224,224))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 2.2009, train acc 0.161, test acc 0.340, time 234.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs,batch_size,ctx=0.1,1,128,d2l.try_gpu()\n",
    "net.initialize(init=init.Xavier(),force_reinit=True,ctx=ctx)\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size,resize=224)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 含并行连接的网络（GoogleNet）\n",
    "在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet的网络结构大放异彩 [1]。它虽然在名字上向LeNet致敬，但在网络结构上已经很难看到LeNet的影子。GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上做了很大改进。在随后的几年里，研究人员对GoogLeNet进行了数次改进，本节将介绍这个模型系列的第一个版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception块\n",
    "GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与上一节介绍的NiN块相比，这个基础块在结构上更加复杂，如图5.8所示。\n",
    "\n",
    "![](http://zh.d2l.ai/_images/inception.svg)\n",
    "由图5.8可以看出，Inception块里有4条并行的线路。前3条线路使用窗口大小分别是 1×1 、 3×3 和 5×5 的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做 1×1 卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用 3×3 最大池化层，后接 1×1 卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。\n",
    "\n",
    "Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import nd,init,gluon\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Block):\n",
    "    #c1-c4为每条线路中的层的输出通道数\n",
    "    def __init__(self,c1,c2,c3,c4,**kwargs):\n",
    "        super(Inception,self).__init__(**kwargs)\n",
    "        \n",
    "        #线路1，1*1卷积层\n",
    "        self.p1_1=nn.Conv2D(c1,kernel_size=1,activation='relu')\n",
    "        \n",
    "        #线路2，1*1卷积和3*3卷积\n",
    "        self.p2_1=nn.Conv2D(c2[0],kernel_size=1,activation='relu')\n",
    "        self.p2_2=nn.Conv2D(c2[1],kernel_size=3,padding=1,strides=1,activation='relu')\n",
    "        \n",
    "        #线路3，1*1卷积和5*5卷积\n",
    "        self.p3_1=nn.Conv2D(c3[0],kernel_size=1,activation='relu')\n",
    "        self.p3_2=nn.Conv2D(c3[1],kernel_size=5,padding=2,strides=1,activation='relu')\n",
    "        \n",
    "        #线路4，3*3 maxpool和1*1卷积\n",
    "        self.p4_1=nn.MaxPool2D(pool_size=3,padding=1,strides=1)\n",
    "        self.p4_2=nn.Conv2D(c4,kernel_size=1,activation='relu')\n",
    "        \n",
    "    def forward(self,X):\n",
    "        p1=self.p1_1(X)\n",
    "        p2=self.p2_2(self.p2_1(X))\n",
    "        p3=self.p3_2(self.p3_1(X))\n",
    "        p4=self.p4_2(self.p4_1(X))\n",
    "        return nd.concat(p1,p2,p3,p4,dim=1)#在通道为上连接\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet模型\n",
    "GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的 3×3 最大池化层来减小输出高宽。第一模块使用一个64通道的 7×7 卷积层。\n",
    "采用inception的GoogLeNet如下：\n",
    "![](https://images2015.cnblogs.com/blog/822124/201609/822124-20160902160824621-1952207446.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=nn.Sequential()\n",
    "b1.add(nn.Conv2D(64,kernel_size=7,strides=2,padding=3,activation='relu'))\n",
    "b1.add(nn.MaxPool2D(pool_size=3,strides=2,padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二模块使用2个卷积层：首先是64通道的 1×1 卷积层，然后是将通道增大3倍的 3×3 卷积层。它对应Inception块中的第二条线路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=nn.Sequential()\n",
    "b2.add(nn.Conv2D(64,kernel_size=1))\n",
    "b2.add(nn.Conv2D(192,kernel_size=3,padding=1)) #未指定激活函数则是线性激活函数f(x)=x，即没有激活函数，直接输出\n",
    "b2.add(nn.MaxPool2D(pool_size=3,strides=2,padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三模块串联2个完整的Inception块。第一个Inception块的输出通道数为64+128+32+32=256，其中4条线路的输出通道数比例为64:128:32:32=2:4:1:1。其中第二、第三条线路先分别将输入通道数减小至96/192=1/2和16/192=1/12后，再接上第二层卷积层。第二个Inception块输出通道数增至128+192+96+64=480，每条线路的输出通道数之比为128:192:96:64=4:6:3:2。其中第二、第三条线路先分别将输入通道数减小至128/256=1/2和32/256=1/8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3=nn.Sequential()\n",
    "b3.add(Inception(64,[96,128],[16,32],32))\n",
    "b3.add(Inception(128,[128,192],[32,96],64))\n",
    "b3.add(nn.MaxPool2D(pool_size=3,strides=2,padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是 192+208+48+64=512 、 160+224+64+64=512 、 128+256+64+64=512 、 112+288+64+64=528 和 256+320+128+128=832 。这些线路的通道数分配和第三模块中的类似，首先含 3×3 卷积层的第二条线路输出最多通道，其次是仅含 1×1 卷积层的第一条线路，之后是含 5×5 卷积层的第三条线路和含 3×3 最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4=nn.Sequential()\n",
    "b4.add(Inception(192,[96,108],[16,48],64))\n",
    "b4.add(Inception(160,[112,224],[24,64],64))\n",
    "b4.add(Inception(128,[128,256],[34,64],64))\n",
    "b4.add(Inception(112,[144,288],[32,64],64))\n",
    "b4.add(Inception(256,[160,320],[32,128],128))\n",
    "b4.add(nn.MaxPool2D(pool_size=3,strides=2,padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第五模块有输出通道数为 256+320+128+128=832 和 384+384+128+128=1024 的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5=nn.Sequential()\n",
    "b5.add(Inception(256,[160,320],[32,128],128))\n",
    "b5.add(Inception(384,[192,384],[48,128],128))\n",
    "b5.add(nn.GlobalAvgPool2D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential()\n",
    "net.add(b1)\n",
    "net.add(b2)\n",
    "net.add(b3)\n",
    "net.add(b4)\n",
    "net.add(b5)\n",
    "net.add(nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。本节里我们将输入的高和宽从224降到96来简化计算。下面演示各个模块之间的输出的形状变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential11 output shape: (1, 64, 24, 24)\n",
      "sequential12 output shape: (1, 192, 12, 12)\n",
      "sequential13 output shape: (1, 480, 6, 6)\n",
      "sequential14 output shape: (1, 832, 3, 3)\n",
      "sequential15 output shape: (1, 1024, 1, 1)\n",
      "dense1 output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X=nd.random.uniform(shape=(1,1,96,96))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 2.2432, train acc 0.200, test acc 0.483, time 173.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs,batch_size,ctx=0.1,1,256,d2l.try_gpu()\n",
    "net.initialize(ctx=ctx,force_reinit=True,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size,resize=96)\n",
    "d2l.train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
